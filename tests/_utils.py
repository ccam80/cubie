import numpy as np

def calculate_expected_summaries(state,
                                 observables,
                                 summarise_every,
                                 output_types,
                                 summary_height_per_variable,
                                 precision,
                                 ):
    """Helper function to calculate expected summary values from a given pair of state and observable arrays.
    Summarises the whole output state and observable array, select from within this if testing for selective
    summarisation.

    Arguments:
    - state_output: 2D array of shape (summary_samples, n_saved_states) output generated by system.
    - observables_output: 2D array of shape (summary_samples, n_saved_observables) output generated by system.
    - summarise_every: Number of samples to summarise over (batch size)
    - output_types: List of output function names to apply (e.g. ["mean", "peaks[3]", "max", "rms"])
    - precision: Numpy dtype to use for the output arrays (e.g. np.float32 or np.float64)

    Returns:
    - expected_state_summaries: 2D array of shape (summary_samples, n_saved_states * summary_size_per_state)
    - expected_obs_summaries: 2D array of shape (summary_samples, n_saved_observables * summary_size_per_state)
        """

    n_saved_states = state.shape[1]
    n_saved_observables = observables.shape[1]
    saved_samples = state.shape[0]
    summary_samples = int(saved_samples / summarise_every)

    state_summaries_height = summary_height_per_variable * n_saved_states
    obs_summaries_height = summary_height_per_variable * n_saved_observables

    expected_state_summaries = np.zeros((summary_samples, state_summaries_height), dtype=precision)
    expected_obs_summaries = np.zeros((summary_samples, obs_summaries_height), dtype=precision)

    for output in output_types:
        if output.startswith('peaks'):
            n_peaks = int(output[6:-1]) if len(output) > 6 else 0
        else:
            n_peaks = 0

 
    for (_input_array, _output_array) in ((state, expected_state_summaries),
                                          (observables, expected_obs_summaries)
                                          ):
        calculate_single_summary_array(_input_array,
                                       summarise_every,
                                       summary_height_per_variable,
                                       output_types,
                                       output_array = _output_array,
                                       n_peaks=n_peaks
                                       )

    return expected_state_summaries, expected_obs_summaries


def calculate_single_summary_array(input_array,
                                   summarise_every,
                                   summary_size_per_state,
                                   output_functions_list,
                                   output_array,
                                   n_peaks=0,
                                   ):
    """ Summarise states in input array in the same way that the device functions do.

    Arguments:
    - input_array: 2D array of shape (n_items, n_samples) with the input data to summarise
    - summarise_every: Number of samples to summarise over
    - summary_size_per_state: Number of summary values per state (e.g. 1 for mean, 1 + n_peaks for mean and peaks[n])
    - output_functions_list: List of output function names to apply (e.g. ["mean", "peaks[3]", "max", "rms"])
    - n_peaks: Number of peaks to find in the "peaks[n]" output function
    - output_array: 2D array to store the summarised output, shape (n_items * summary_size_per_state, n_samples)

    Returns:
    - None, but output_array is filled with the summarised values.

        """
    summary_samples = int(input_array.shape[0] / summarise_every)
    try:
        n_items = output_array.shape[1] // summary_size_per_state
    except ZeroDivisionError:
        n_items = 0

    # Manual cycling through possible summaries_array to match the approach used when building the device functions
    for j in range(n_items):
        for i in range(summary_samples):
            summary_index = 0
            for output_type in output_functions_list:
                start_index = i * summarise_every
                end_index = (i + 1) * summarise_every
                if output_type == 'mean':
                    output_array[i, j * summary_size_per_state + summary_index] = np.mean(
                            input_array[start_index: end_index, j], axis=0,
                            )
                    summary_index += 1

                if output_type.startswith('peaks'):
                    # Use the last two samples, like the live version does
                    start_index = i * summarise_every - 2 if i > 0 else 0
                    maxima = local_maxima(
                            input_array[start_index: end_index, j],
                            )[:n_peaks] + start_index
                    output_start_index = j * summary_size_per_state + summary_index
                    output_array[i, output_start_index:output_start_index + maxima.size] = maxima
                    summary_index += n_peaks

                if output_type == 'max':
                    _max = np.max(input_array[ start_index: end_index, j], axis=0)
                    output_array[i, j * summary_size_per_state + summary_index] = _max
                    summary_index += 1

                if output_type == 'rms':
                    rms = np.sqrt(np.mean(input_array[start_index: end_index, j] ** 2, axis=0))
                    output_array[i, j * summary_size_per_state + summary_index] = rms
                    summary_index += 1


def local_maxima(signal: np.ndarray) -> np.ndarray:
    return np.flatnonzero((signal[1:-1] > signal[:-2]) & (signal[1:-1] > signal[2:])) + 1


def cpu_euler_loop(system,
               inits,
               params,
               driver_vec,
               dt,
               output_dt,
               warmup,
               duration,
               saved_observable_indices,
               saved_state_indices,
               save_time,
               ):
    """A simple CPU implementation of the Euler loop for testing."""
    t = 0.0
    save_every = int(round(output_dt / dt))
    output_length = int(duration / output_dt)
    warmup_samples = int(warmup / output_dt)
    n_saved_states = len(saved_state_indices)
    n_saved_observables = len(saved_observable_indices)
    total_samples = int((duration + warmup) / output_dt)

    state_output = np.zeros((output_length, n_saved_states + save_time * 1), dtype=inits.dtype)
    observables_output = np.zeros((output_length, n_saved_observables), dtype=inits.dtype)
    state = inits.copy()

    for i in range(total_samples):
        for j in range(save_every):
            drivers = driver_vec[(i * save_every + j) % len(driver_vec), :]
            t += dt
            dx, observables = system.correct_answer_python(state, params, drivers)
            state += dx * dt
        if i > (warmup_samples - 1):
            state_output[i - warmup_samples, :n_saved_states] = state[saved_state_indices]
            observables_output[i - warmup_samples, :] = observables[saved_observable_indices]
            if save_time:
                state_output[i - warmup_samples, -1] = i - warmup_samples

    return state_output, observables_output

### ********************************************************************************************************* ###
#                                        RANDOM GENERATION
### ********************************************************************************************************* ###
def single_scale_float_array(shape: int | tuple[int], precision=np.float64, scale=1e6):
    """Generate a random float array of given shape and dtype, drawn from a normal distribution with a std dev of the
    argument "scale". Normal was chosen here to slightly increase the magnitude-spead of values.

    Args:
        shape (tuple[int] | int): The shape of the array to generate.
        precision (np.dtype): The desired data type of the array.
        scale (float): The standard deviation of the normal distribution from which to draw values.
    Returns:
        random_array (np.ndarray): A numpy array of the specified shape and dtype, filled with random values.

    """
    rng = np.random.default_rng()
    return rng.normal(scale=scale, size=shape).astype(precision)


def mixed_scale_float_array(shape: int | tuple[int],
                            precision=np.float64,
                            log10_scale=(-6, 6),
                            axis=0,
                            ):
    """ Generates a float array where each element is drawn from a normal distribution. The std dev of the distribution
    is 1*10^k, with drawn from a uniform distribution between log10_scale[0] and log10_scale[1]. The resulting array
    can be used to test the system with a wide dynamic range of values, straining the numerical stability of the system.

    Args:
        shape (tuple[int] | int): The shape of the array to generate.
        precision (np.dtype): The desired data type of the array. default: np.float64.
        log10_scale (tuple[float]): A tuple of (min_exponent, max_exponent) two floats, the lower and upper bounds of
            the log10 scale for the standard deviation. default: (-6, 6).
        axis (int): all values along this axis will be drawn from a distribution of the same scale - in the context of
            an ODE system, this means that each state will contain values at the same scale, so set it to the index
            that corresponds to the state/parameter/value. default: 0

    Returns:
        random_array (np.ndarray): A numpy array of the specified shape and dtype, filled with random values drawn from
            normal distributions with varying scales.

    """
    rng = np.random.default_rng()
    if isinstance(shape, int):
        shape = (shape,)
    if axis > len(shape):
        raise ValueError(f"Axis {axis} is out of bounds for shape {shape}.")
    scale_exponents = rng.uniform(log10_scale[0], log10_scale[1], size=shape[axis])
    scale_values = 10.0 ** scale_exponents
    _random_array = np.empty(shape, dtype=precision)
    for i in range(shape[axis]):
        _random_array[i] = rng.normal(scale=scale_values[i], size=shape[:axis] + shape[axis + 1:]).astype(precision)
    return _random_array


def random_array(precision, size: int | tuple[int], scale=1e6):
    """Generate a random float array of given size and dtype, drawn from a normal distribution with a std dev of the
    argument "scale". Normal was chosen here to slightly increase the magnitude-spead of values.

    Args:
        precision (np.dtype): The desired data type of the array.
        size (int): The size of the array to generate.
        scale (float): The standard deviation of the normal distribution from which to draw values.
    Returns:
        random_array (np.ndarray): A numpy array of the specified size and dtype, filled with random values.

    """
    if isinstance(scale, float):
        scale = (scale,)
    if len(scale) == 1:
        randvals = single_scale_float_array(size, precision, scale[0])
    elif len(scale) == 2:
        randvals = mixed_scale_float_array(size, precision, log10_scale=scale, axis=0)
    else:
        raise ValueError(f"scale must be a single float or a tuple of two floats, got {scale}.")

    return randvals


def nan_array(precision, size):
    """Generate an array of NaNs of given size and dtype.

    Args:
        precision (np.dtype): The desired data type of the array.
        size (int): The size of the array to generate.
    Returns:
        nan_array (np.ndarray): A numpy array of the specified size and dtype, filled with NaN values.
    """
    return np.full(size, np.nan, dtype=precision)


def zero_array(precision, size):
    """Generate an array of zeros of given size and dtype.

    Args:
        precision (np.dtype): The desired data type of the array.
        size (int): The size of the array to generate.
    Returns:
        zero_array (np.ndarray): A numpy array of the specified size and dtype, filled with zeros.
    """
    return np.zeros(size, dtype=precision)


def ones_array(precision, size):
    """Generate an array of ones of given size and dtype.

    Args:
        precision (np.dtype): The desired data type of the array.
        size (int): The size of the array to generate.
    Returns:
        one_array (np.ndarray): A numpy array of the specified size and dtype, filled with ones.
    """
    return np.ones(size, dtype=precision)


def generate_test_array(precision, size, style, scale=None):
    """Generate a test array of given size and dtype, with the specified type.

    Args:
        precision (np.dtype): The desired data type of the array.
        size (int | tuple[int]): The size of the array to generate.
        style (str): The type of array to generate. Options: 'random', 'nan', 'zero', 'ones'.
        scale (float | tuple[float]): The scale for the random array, if type is 'random'. Default: None.
    Returns:
        test_array (np.ndarray): A numpy array of the specified size and dtype, filled with values according to the type.
    """
    if style == 'random':
        if scale is None:
            raise ValueError("scale must be specified if type is 'random'.")
        return random_array(precision, size, scale)
    elif style == 'nan':
        return nan_array(precision, size)
    elif style == 'zero':
        return zero_array(precision, size)
    elif style == 'ones':
        return ones_array(precision, size)
    else:
        raise ValueError(f"Unknown array type: {style}. Use 'random', 'nan', 'zero', or 'ones'.")