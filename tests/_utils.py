from __future__ import annotations

from dataclasses import dataclass
import math
from typing import Mapping, Optional

import numpy as np
from numba import cuda, from_dtype
from numpy.testing import assert_allclose

from cubie.outputhandling import OutputFunctions
from cubie.integrators.array_interpolator import ArrayInterpolator
from cubie.integrators.loops.ode_loop import IVPLoop
from cubie.odesystems.baseODE import BaseODE
from cubie.outputhandling import OutputArrayHeights
from numpy.typing import NDArray

Array = NDArray[np.floating]


def calculate_expected_summaries(
    state,
    observables,
    summarised_state_indices,
    summarised_observable_indices,
    summarise_every,
    output_types,
    summary_height_per_variable,
    precision,
):
    """Helper function to calculate expected summary values from a given pair of state and observable arrays.
    Summarises the whole output state and observable array, select from within this if testing for selective
    summarisation.

    Arguments:
    - state_output: 2D array of shape (summary_samples, n_saved_states) output generated by system.
    - observables_output: 2D array of shape (summary_samples, n_saved_observables) output generated by system.
    - summarise_every: Number of samples to summarise over (batch size)
    - output_types: List of output function names to apply (e.g. ["mean", "peaks[3]", "max", "rms"])
    - precision: Numpy dtype to use for the output arrays (e.g. np.float32 or np.float64)

    Returns:
    - expected_state_summaries: 2D array of shape (summary_samples, n_saved_states * summary_size_per_state)
    - expected_obs_summaries: 2D array of shape (summary_samples, n_saved_observables * summary_size_per_state)
    """
    state = state[:,summarised_state_indices]
    observables = observables[:,summarised_observable_indices]
    n_saved_states = state.shape[1]
    n_saved_observables = observables.shape[1]
    saved_samples = state.shape[0]
    summary_samples = int(saved_samples / summarise_every)

    state_summaries_height = summary_height_per_variable * n_saved_states
    obs_summaries_height = summary_height_per_variable * n_saved_observables

    expected_state_summaries = np.zeros(
        (summary_samples, state_summaries_height), dtype=precision
    )
    expected_obs_summaries = np.zeros(
        (summary_samples, obs_summaries_height), dtype=precision
    )

    for output in output_types:
        if output.startswith("peaks") or output.startswith("negative_peaks"):
            n_peaks = int(output.split('[')[1].split(']')[0]) if '[' in output else 0
        else:
            n_peaks = 0

    for _input_array, _output_array in (
        (state, expected_state_summaries),
        (observables, expected_obs_summaries),
    ):
        calculate_single_summary_array(
            _input_array,
            summarise_every,
            summary_height_per_variable,
            output_types,
            output_array=_output_array,
            n_peaks=n_peaks,
        )

    return expected_state_summaries, expected_obs_summaries


def calculate_single_summary_array(
    input_array,
    summarise_every,
    summary_size_per_state,
    output_functions_list,
    output_array,
    n_peaks=0,
):
    """Summarise states in input array in the same way that the device functions do.

    Arguments:
    - input_array: 2D array of shape (n_items, n_samples) with the input data to summarise
    - summarise_every: Number of samples to summarise over
    - summary_size_per_state: Number of summary values per state (e.g. 1 for mean, 1 + n_peaks for mean and peaks[n])
    - output_functions_list: List of output function names to apply (e.g. ["mean", "peaks[3]", "max", "rms"])
    - n_peaks: Number of peaks to find in the "peaks[n]" output function
    - output_array: 2D array to store the summarised output, shape (n_items * summary_size_per_state, n_samples)

    Returns:
    - None, but output_array is filled with the summarised values.

    """
    summary_samples = int(input_array.shape[0] / summarise_every)
    try:
        n_items = output_array.shape[1] // summary_size_per_state
    except ZeroDivisionError:
        n_items = 0

    # Manual cycling through possible summaries_array to match the approach used when building the device functions
    for j in range(n_items):
        for i in range(summary_samples):
            summary_index = 0
            for output_type in output_functions_list:
                start_index = i * summarise_every
                end_index = (i + 1) * summarise_every
                if output_type == "mean":
                    output_array[
                        i, j * summary_size_per_state + summary_index
                    ] = np.mean(
                        input_array[start_index:end_index, j],
                        axis=0,
                    )
                    summary_index += 1

                if output_type.startswith("peaks"):
                    # Use the last two samples, like the live version does
                    start_index = i * summarise_every - 2 if i > 0 else 0
                    maxima = (
                        local_maxima(
                            input_array[start_index:end_index, j],
                        )[:n_peaks]
                        + start_index
                    )
                    output_start_index = (
                        j * summary_size_per_state + summary_index
                    )
                    output_array[
                        i,
                        output_start_index : output_start_index + maxima.size,
                    ] = maxima
                    summary_index += n_peaks

                if output_type == "max":
                    _max = np.max(
                        input_array[start_index:end_index, j], axis=0
                    )
                    output_array[
                        i, j * summary_size_per_state + summary_index
                    ] = _max
                    summary_index += 1

                if output_type == "rms":
                    rms = np.sqrt(
                        np.mean(
                            input_array[start_index:end_index, j] ** 2, axis=0
                        )
                    )
                    output_array[
                        i, j * summary_size_per_state + summary_index
                    ] = rms
                    summary_index += 1

                if output_type == "std":
                    std = np.std(
                        input_array[start_index:end_index, j], axis=0
                    )
                    output_array[
                        i, j * summary_size_per_state + summary_index
                    ] = std
                    summary_index += 1

                if output_type == "min":
                    _min = np.min(
                        input_array[start_index:end_index, j], axis=0
                    )
                    output_array[
                        i, j * summary_size_per_state + summary_index
                    ] = _min
                    summary_index += 1

                if output_type == "max_magnitude":
                    max_mag = np.max(
                        np.abs(input_array[start_index:end_index, j]), axis=0
                    )
                    output_array[
                        i, j * summary_size_per_state + summary_index
                    ] = max_mag
                    summary_index += 1

                if output_type == "extrema":
                    _max = np.max(
                        input_array[start_index:end_index, j], axis=0
                    )
                    _min = np.min(
                        input_array[start_index:end_index, j], axis=0
                    )
                    output_array[
                        i, j * summary_size_per_state + summary_index
                    ] = _max
                    output_array[
                        i, j * summary_size_per_state + summary_index + 1
                    ] = _min
                    summary_index += 2

                if output_type.startswith("negative_peaks"):
                    # Use the last two samples, like the live version does
                    start_index = i * summarise_every - 2 if i > 0 else 0
                    minima = (
                        local_minima(
                            input_array[start_index:end_index, j],
                        )[:n_peaks]
                        + start_index
                    )
                    output_start_index = (
                        j * summary_size_per_state + summary_index
                    )
                    output_array[
                        i,
                        output_start_index : output_start_index + minima.size,
                    ] = minima
                    summary_index += n_peaks

                if output_type == "mean_std_rms":
                    _mean = np.mean(
                        input_array[start_index:end_index, j], axis=0
                    )
                    _std = np.std(
                        input_array[start_index:end_index, j], axis=0
                    )
                    _rms = np.sqrt(
                        np.mean(
                            input_array[start_index:end_index, j] ** 2, axis=0
                        )
                    )
                    output_array[
                        i, j * summary_size_per_state + summary_index
                    ] = _mean
                    output_array[
                        i, j * summary_size_per_state + summary_index + 1
                    ] = _std
                    output_array[
                        i, j * summary_size_per_state + summary_index + 2
                    ] = _rms
                    summary_index += 3

                if output_type == "mean_std":
                    _mean = np.mean(
                        input_array[start_index:end_index, j], axis=0
                    )
                    _std = np.std(
                        input_array[start_index:end_index, j], axis=0
                    )
                    output_array[
                        i, j * summary_size_per_state + summary_index
                    ] = _mean
                    output_array[
                        i, j * summary_size_per_state + summary_index + 1
                    ] = _std
                    summary_index += 2

                if output_type == "std_rms":
                    _std = np.std(
                        input_array[start_index:end_index, j], axis=0
                    )
                    _rms = np.sqrt(
                        np.mean(
                            input_array[start_index:end_index, j] ** 2, axis=0
                        )
                    )
                    output_array[
                        i, j * summary_size_per_state + summary_index
                    ] = _std
                    output_array[
                        i, j * summary_size_per_state + summary_index + 1
                    ] = _rms
                    summary_index += 2


def local_maxima(signal: np.ndarray) -> np.ndarray:
    return (
        np.flatnonzero(
            (signal[1:-1] > signal[:-2]) & (signal[1:-1] > signal[2:])
        )
        + 1
    )


def local_minima(signal: np.ndarray) -> np.ndarray:
    return (
        np.flatnonzero(
            (signal[1:-1] < signal[:-2]) & (signal[1:-1] < signal[2:])
        )
        + 1
    )


### ********************************************************************************************************* ###
#                                        RANDOM GENERATION
### ********************************************************************************************************* ###
def single_scale_float_array(
    shape: int | tuple[int], precision=np.float64, scale=1e6
):
    """Generate a random float array of given shape and dtype, drawn from a normal distribution with a std dev of the
    argument "scale". Normal was chosen here to slightly increase the magnitude-spead of values.

    Args:
        shape (tuple[int] | int): The shape of the array to generate.
        precision (np.dtype): The desired data type of the array.
        scale (float): The standard deviation of the normal distribution from which to draw values.
    Returns:
        random_array (np.ndarray): A numpy array of the specified shape and dtype, filled with random values.

    """
    rng = np.random.default_rng()
    return rng.normal(scale=scale, size=shape).astype(precision)


def mixed_scale_float_array(
    shape: int | tuple[int],
    precision=np.float64,
    log10_scale=(-6, 6),
    axis=0,
):
    """Generates a float array where each element is drawn from a normal distribution. The std dev of the distribution
    is 1*10^k, with drawn from a uniform distribution between log10_scale[0] and log10_scale[1]. The resulting array
    can be used to test the system with a wide dynamic range of values, straining the numerical stability of the system.

    Args:
        shape (tuple[int] | int): The shape of the array to generate.
        precision (np.dtype): The desired data type of the array. default: np.float64.
        log10_scale (tuple[float]): A tuple of (min_exponent, max_exponent) two floats, the lower and upper bounds of
            the log10 scale for the standard deviation. default: (-6, 6).
        axis (int): all values along this axis will be drawn from a distribution of the same scale - in the context of
            an ODE system, this means that each state will contain values at the same scale, so set it to the index
            that corresponds to the state/parameter/value. default: 0

    Returns:
        random_array (np.ndarray): A numpy array of the specified shape and dtype, filled with random values drawn from
            normal distributions with varying scales.

    """
    rng = np.random.default_rng()
    if isinstance(shape, int):
        shape = (shape,)
    if axis > len(shape):
        raise ValueError(f"Axis {axis} is out of bounds for shape {shape}.")
    scale_exponents = rng.uniform(
        log10_scale[0], log10_scale[1], size=shape[axis]
    )
    scale_values = 10.0**scale_exponents
    _random_array = np.empty(shape, dtype=precision)
    for i in range(shape[axis]):
        _random_array[i] = rng.normal(
            scale=scale_values[i], size=shape[:axis] + shape[axis + 1 :]
        ).astype(precision)
    return _random_array


def random_array(precision, size: int | tuple[int], scale=1e6):
    """Generate a random float array of given size and dtype, drawn from a normal distribution with a std dev of the
    argument "scale". Normal was chosen here to slightly increase the magnitude-spead of values.

    Args:
        precision (np.dtype): The desired data type of the array.
        size (int): The size of the array to generate.
        scale (float): The standard deviation of the normal distribution from which to draw values.
    Returns:
        random_array (np.ndarray): A numpy array of the specified size and dtype, filled with random values.

    """
    if isinstance(scale, float):
        scale = (scale,)
    if len(scale) == 1:
        randvals = single_scale_float_array(size, precision, scale[0])
    elif len(scale) == 2:
        randvals = mixed_scale_float_array(
            size, precision, log10_scale=scale, axis=0
        )
    else:
        raise ValueError(
            f"scale must be a single float or a tuple of two floats, got {scale}."
        )

    return randvals


def nan_array(precision, size):
    """Generate an array of NaNs of given size and dtype.

    Args:
        precision (np.dtype): The desired data type of the array.
        size (int): The size of the array to generate.
    Returns:
        nan_array (np.ndarray): A numpy array of the specified size and dtype, filled with NaN values.
    """
    return np.full(size, np.nan, dtype=precision)


def zero_array(precision, size):
    """Generate an array of zeros of given size and dtype.

    Args:
        precision (np.dtype): The desired data type of the array.
        size (int): The size of the array to generate.
    Returns:
        zero_array (np.ndarray): A numpy array of the specified size and dtype, filled with zeros.
    """
    return np.zeros(size, dtype=precision)


def ones_array(precision, size):
    """Generate an array of ones of given size and dtype.

    Args:
        precision (np.dtype): The desired data type of the array.
        size (int): The size of the array to generate.
    Returns:
        one_array (np.ndarray): A numpy array of the specified size and dtype, filled with ones.
    """
    return np.ones(size, dtype=precision)


def generate_test_array(precision, size, style, scale=None):
    """Generate a test array of given size and dtype, with the specified type.

    Args:
        precision (np.dtype): The desired data type of the array.
        size (int | tuple[int]): The size of the array to generate.
        style (str): The type of array to generate. Options: 'random', 'nan', 'zero', 'ones'.
        scale (float | tuple[float]): The scale for the random array, if type is 'random'. Default: None.
    Returns:
        test_array (np.ndarray): A numpy array of the specified size and dtype, filled with values according to the type.
    """
    if style == "random":
        if scale is None:
            raise ValueError("scale must be specified if type is 'random'.")
        return random_array(precision, size, scale)
    elif style == "nan":
        return nan_array(precision, size)
    elif style == "zero":
        return zero_array(precision, size)
    elif style == "ones":
        return ones_array(precision, size)
    else:
        raise ValueError(
            f"Unknown array type: {style}. Use 'random', 'nan', 'zero', or 'ones'."
        )

# ******************** Device Test Kernels *********************************  #
@dataclass
class LoopRunResult:
    """Container holding the outputs produced by a single loop execution."""

    state: Array
    observables: Array
    state_summaries: Array
    observable_summaries: Array
    status: int


def run_device_loop(
    *,
    loop: IVPLoop,
    system: BaseODE,
    initial_state: Array,
    output_functions: OutputFunctions,
    solver_config: Mapping[str, float],
    localmem_required: int = 0,
    sharedmem_required: int = 0,
    driver_array: Optional[ArrayInterpolator] = None,

) -> LoopRunResult:
    """Execute ``loop`` on the CUDA simulator and return host-side outputs."""

    precision = loop.precision
    dt_save = loop.dt_save
    warmup = solver_config['warmup']
    duration = solver_config["duration"]
    total_time = warmup + duration
    save_samples = int(np.ceil(precision(total_time) / precision(dt_save)))

    heights = OutputArrayHeights.from_output_fns(output_functions)

    state_width = max(heights.state, 1)
    observable_width = max(heights.observables, 1)
    state_summary_width = max(heights.state_summaries, 1)
    observable_summary_width = max(heights.observable_summaries, 1)

    state_output = np.zeros((save_samples, state_width), dtype=precision)
    observables_output = np.zeros(
        (save_samples, observable_width), dtype=precision
    )

    summarise_dt = loop.dt_summarise
    summary_samples = int(np.ceil(duration / summarise_dt))

    state_summary_output = np.zeros(
        (summary_samples, state_summary_width), dtype=precision
    )
    observable_summary_output = np.zeros(
        (summary_samples, observable_summary_width), dtype=precision
    )

    params = np.array(
        system.parameters.values_array,
        dtype=precision,
        copy=True,
    )
    init_state = np.array(initial_state, dtype=precision, copy=True)
    status = np.zeros(1, dtype=np.int32)

    d_init = cuda.to_device(init_state)
    d_params = cuda.to_device(params)
    if driver_array is None:
        order = int(solver_config["driverspline_order"])
        width = min(system.num_drivers, 1)
        coeff_shape = (1, width, order + 1)
        driver_coefficients = np.zeros(coeff_shape, dtype=precision)
    else:
        driver_coefficients = np.array(
            driver_array.coefficients, dtype=precision, copy=True
        )
    d_driver_coeffs = cuda.to_device(driver_coefficients)
    d_state_out = cuda.to_device(state_output)
    d_obs_out = cuda.to_device(observables_output)
    d_state_sum = cuda.to_device(state_summary_output)
    d_obs_sum = cuda.to_device(observable_summary_output)
    d_status = cuda.to_device(status)

    shared_elements = sharedmem_required
    shared_bytes = np.dtype(precision).itemsize * shared_elements

    loop_fn = loop.device_function
    numba_precision = from_dtype(precision)

    @cuda.jit
    def kernel(
        init_vec,
        params_vec,
        driver_coeffs_vec,
        state_out_arr,
        obs_out_arr,
        state_sum_arr,
        obs_sum_arr,
        status_arr,
    ):
        idx = cuda.grid(1)
        if idx > 0:
            return

        shared = cuda.shared.array(0, dtype=numba_precision)
        shared[:] = numba_precision(0.0)
        local = cuda.local.array(localmem_required, dtype=numba_precision)
        local[:] = numba_precision(0.0)
        status_arr[0] = loop_fn(
            init_vec,
            params_vec,
            driver_coeffs_vec,
            shared,
            local,
            state_out_arr,
            obs_out_arr,
            state_sum_arr,
            obs_sum_arr,
            precision(duration),
            precision(warmup),
            precision(0.0),
        )

    kernel[1, 1, 0, shared_bytes](
        d_init,
        d_params,
        d_driver_coeffs,
        d_state_out,
        d_obs_out,
        d_state_sum,
        d_obs_sum,
        d_status,
    )
    cuda.synchronize()

    state_host = d_state_out.copy_to_host()
    observables_host = d_obs_out.copy_to_host()
    state_summary_host = d_state_sum.copy_to_host()
    observable_summary_host = d_obs_sum.copy_to_host()
    status_value = int(d_status.copy_to_host()[0])

    return LoopRunResult(
        state=state_host,
        observables=observables_host,
        state_summaries=state_summary_host,
        observable_summaries=observable_summary_host,
        status=status_value,
    )


def assert_integration_outputs(
    reference,
    device,
    output_functions,
    rtol: float,
    atol: float,
) -> None:
    """Compare state, summary, and time outputs between CPU and device."""
    if isinstance(reference, dict):
        reference = LoopRunResult(**reference)
    flags = output_functions.compile_flags

    state_ref, time_ref = extract_state_and_time(
        reference.state, output_functions
    )
    state_dev, time_dev = extract_state_and_time(
        device.state,
        output_functions,
    )
    observables_ref = reference.observables
    observables_dev = device.observables

    if output_functions.save_time:
        assert_allclose(
            time_dev,
            time_ref,
            rtol=rtol,
            atol=atol,
            err_msg="time mismatch.\n"
            f"device: {time_dev}\nreference: {time_ref}",
        )

    if flags.save_state:
            assert_allclose(
                state_dev,
                state_ref,
                rtol=rtol,
                atol=atol,
                verbose=True,
                err_msg="state mismatch.\n"
                f"device: {state_dev}\nreference: {state_ref}\ndelta (ref - "
                        f"dev): {state_ref - state_dev}\n",
            )


    if flags.save_observables:
        assert_allclose(
            observables_dev,
            observables_ref,
            rtol=rtol,
            atol=atol,
            err_msg="observables mismatch.\n"
            f"device: {observables_dev}\n"
            f"reference: {observables_ref}",
        )

    if flags.summarise_state:
        assert_allclose(
            device.state_summaries,
            reference.state_summaries,
            rtol=rtol,
            atol=atol,
            err_msg="state summaries mismatch.\n"
                    f"device: {device.state_summaries}\n"
                    f"reference: {reference.state_summaries}")

    if flags.summarise_observables:
        assert_allclose(
            device.observable_summaries,
            reference.observable_summaries,
            rtol=rtol,
            atol=atol,
            err_msg="observable summary mismatch.\n"
            f"device: {device.observable_summaries}\n"
            f"reference: {reference.observable_summaries}",
        )


def extract_state_and_time(
    state_output: Array, output_functions: OutputFunctions
) -> tuple[Array, Optional[Array]]:
    """Split state output into state variables and optional time column."""
    n_state_columns = output_functions.n_saved_states
    if not output_functions.save_time:
        return state_output, None
    if state_output.ndim == 2:
        state_values = state_output[:, :n_state_columns]
        time_values = state_output[:, n_state_columns : n_state_columns + 1]
    else:
        state_values = state_output[:, :, :n_state_columns]
        time_values = state_output[:, :, n_state_columns : ]

    return state_values, time_values


def _driver_sequence(
    *,
    samples: int,
    total_time: float,
    n_drivers: int,
    precision,
) -> Array:
    """Drive system with a sine wave."""

    width = max(n_drivers, 1)
    drivers = np.zeros((samples, width), dtype=precision)
    if n_drivers > 0 and total_time > 0.0:
        times = np.linspace(0.0, total_time, samples, dtype=precision)
        for idx in range(n_drivers):
            drivers[:, idx] = precision(
                1.0 + np.sin(2 * np.pi * (idx + 1) * times / total_time))
    return drivers


def evaluate_driver_series(
    driver: ArrayInterpolator,
    time: float,
    precision,
) -> Array:
    """Evaluate driver splines on the host at ``time``."""

    coeffs = np.asarray(driver.coefficients)
    if coeffs.size == 0:
        width = max(driver.num_drivers, 1)
        return np.zeros(width, dtype=precision)

    resolution = precision(driver.dt)
    start_time = precision(driver.t0)
    wrap = bool(driver.wrap)
    num_segments = coeffs.shape[0]
    inv_res = 1.0 / resolution if resolution != 0.0 else 0.0

    scaled = (time - start_time) * inv_res if resolution != 0.0 else 0.0
    segment = math.floor(scaled)
    if wrap and num_segments > 0:
        segment %= num_segments
        if segment < 0:
            segment += num_segments
    else:
        segment = max(0, min(num_segments - 1, segment))

    base_time = start_time + resolution * precision(segment)
    tau = (time - base_time) * inv_res if resolution != 0.0 else 0.0

    values = np.zeros(coeffs.shape[1], dtype=precision)
    for driver_idx in range(coeffs.shape[1]):
        segment_coeffs = coeffs[segment, driver_idx]
        acc = 0.0
        for power in range(segment_coeffs.size - 1, -1, -1):
            acc = acc * tau + precision(segment_coeffs[power])
        values[driver_idx] = precision(acc)
    return values
